\documentclass[11pt]{article}
\input{preamble}
\begin{document}
\thispagestyle{empty}
{\LARGE \bf MAT 185 Lecture Notes}\\
{\large Hei Shing Cheung}\\
Linear Algebra, Winter 2024 \hfill MAT 185\\
\\
The up-to-date version of this document can be found at \url{https://github.com/HaysonC/skulenotes}\\
\section{Vector Space}
\subsection{Foundamental Properties}
\begin{definition}[Vector Space in $\mathbb{R}$, Field]
    This course concerns with real vector spaces. A vectors space, ${V}$, over $\mathbb{R}$ is a collection of \textbf{object} $\textbf{v} \in V$ s.t. the follow axioms are followed
    \begin{enumerate}
        \item \textbf{Addition Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Addition:} \\
            $\textbf{x}, \textbf{y} \in V \implies \textbf{x} + \textbf{y} \in V$
            
            \item \textbf{Associativity of Addition:} \\
            $\textbf{x}, \textbf{y}, \textbf{z} \in V \implies (\textbf{x} + \textbf{y}) + \textbf{z} = \textbf{x} + (\textbf{y} + \textbf{z})$
            
            \item \textbf{Existence of Additive Identity:} \\
            $\exists \, \textbf{0} \in V$ such that $\textbf{x} \in V \implies \textbf{x} + \textbf{0} = \textbf{x}$
            
            \item \textbf{Existence of Additive Inverse:} \\
            $\forall \, \textbf{x} \in V, \, \exists \, \textbf{-x} \in V$ such that $\textbf{x} + \textbf{-x} = \textbf{0}$
        \end{enumerate}
        
        \item \textbf{Scalar Multiplication Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha \in \mathbb{R}, \, \alpha \textbf{x} \in V$
            
            \item \textbf{Associativity of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha \beta) \textbf{x} = \alpha (\beta \textbf{x})$
            
            \item \textbf{Distributive Property of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha + \beta) \textbf{x} = \alpha \textbf{x} + \beta \textbf{x}$
            
            \item \textbf{Existence of Multiplicative Identity:} \\
            $\forall \, \textbf{x} \in V, \, \textbf{1} \textbf{x} = \textbf{x}$
        \end{enumerate}
    \end{enumerate}
     
    \paragraph{Note} It could be shown that the axiom imply the commutativity of in addition, namely $\forall \textbf{x}, \textbf{y} \in \mathbb{R}$, $\textbf{x} + \textbf{y} = \textbf{y} + \textbf{x}$
\end{definition}
\paragraph{Example} $\mathbb{R}^n$ is a vector space over $\mathbb{R}$, where $\textbf{x} = (x_1, x_2, \ldots, x_n)$ and $\textbf{y} = (y_1, y_2, \ldots, y_n)$, then $\textbf{x} + \textbf{y} = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)$ and $\alpha\textbf{x} = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n)$
\begin{theorem}[Cancelation, Part 1]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\ 
        \intertext{From additive inverse we know that \textbf{-z} exists}
        (\textbf{x} + \textbf{z}) + \textbf{-z} &= (\textbf{y} + \textbf{z}) + \textbf{-z} \\
        \intertext{By order of addition we have: }
        \textbf{x} + (\textbf{z} + \textbf{-z}) &= \textbf{y} + (\textbf{z} + \textbf{-z}) \\
        \textbf{x} + \textbf{0} &= \textbf{y} + \textbf{0} \\
        \intertext{By additive identity we have: }
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{theorem}[Cancelation, Part 2]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
To proof that, it would require the following propostion:
\begin{lemma}
    Let $V$ be a vector space and $\textbf{z} \in V$, then $\textbf{-z} + \textbf{z} = 0 $
\end{lemma}
\begin{proof}
    \begin{align*}
        \intertext{We know:}
        \textbf{-z} + \textbf{z} &= (\textbf{-z} + \textbf{z}) + 0\\
        &= (\textbf{-z} + \textbf{z}) + (\textbf{-z} + \textbf{-(-z)}) \\
        \intertext{By the order of additon, }
        &= \textbf{-z} + (\textbf{z} + (\textbf{-z} + \textbf{-(-z)})) \\
        \intertext{Again, by the order of additon, }
        &= \textbf{-z} + ((\textbf{z} + \textbf{-z}) + \textbf{-(-z)}) \\
        &= \textbf{-z} + \textbf{-(-z)} \\
        &= 0
    \end{align*}
\end{proof}
Now, to prove the part 2 of the Cancelation Theorem:
\begin{proof}
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \intertext{From additive inverse we know that \textbf{-z} exists}
        \textbf{-z} + (\textbf{z} + \textbf{x}) &= \textbf{-z} + (\textbf{z} + \textbf{y})\\
        (\textbf{-z} + \textbf{z}) + \textbf{x} &= (\textbf{-z} + \textbf{z}) + \textbf{y}\\
        \intertext{From above, we have $\textbf{-z} + \textbf{z} = 0$}
        0 + \textbf{x} &= 0 + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{lemma}[Inverse of an inverse]
    Let $V$ be a vector space and $\textbf{x} \in V$, then:
    \begin{align*}
        \textbf{-(-x)} &= \textbf{x}
    \end{align*}
\end{lemma}
\begin{proof}
    Assume $0$, $0^\star$ are the additive identity of $V$ and $\textbf{-x}$, $\textbf{-x}^\star$ are the additive inverse of $\textbf{x}$. We have:
    \begin{equation*}
        u+0 = u + 0^\star
    \end{equation*}
    By Cancelation Theorem, we have $0 = 0^\star$. Thus, the additive identity is unique. Now, we have:
    \begin{align*}
        \textbf{x} + \textbf{-x} &= 0 \\
        \textbf{x} + \textbf{-x}^\star &= 0
        \intertext{Thus, by the uniqueness of the additive inverse, we have:}
        \textbf{x} + \textbf{-x} &= \textbf{x} + \textbf{-x}^\star
        \intertext{By the Cancelation Theorem, we have:}
        \textbf{-x} &= \textbf{-x}^\star
    \end{align*}
    Now, we have that the additive inverse is unique. Thus, $\textbf{-(-x)}$ must be unique and nessarily $\textbf{x}$. 
\end{proof}
\paragraph{Additional Results from Above}
\begin{enumerate}
    \item The additive identity is unique
    \item The additive inverse is unique
\end{enumerate}
\begin{definition}[Subtraction]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} - \textbf{y} &\defeq \textbf{x} + \textbf{-y}
    \end{align*}
\end{definition}
\begin{theorem}[Addition is Commutative]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{y} &= \textbf{y} + \textbf{x}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{y} &=  
    \end{align*}
\end{proof} 
\subsection{Vector Subspace}
\begin{definition}[Vector Subspace]
    Let $V$ be a vector space and $W \subseteq V$, then $W$ is a vector subspace of $V$ if $W$ is a vector space.
\end{definition}
\begin{theorem}[Subspace Test, I]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha \in \mathbb{R}$:
    \begin{enumerate}
        \item \textbf{Closure Under Addition:} \\
        $\textbf{x} + \textbf{y} \in W$
        \item \textbf{Closure Under Scalar Multiplication:} \\
        $\alpha \textbf{x} \in W$
        \item \text{Additive Identity:} \\
        $\textbf{0} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the three conditions are satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Null Space]
    Let $V$ be a vector space and $A \in {^m\mathbb{R}^n}$\footnote{Same as $\mathbb{R}^{m\times n}$, The set of all $m \times n$ matrices with real entries}, then:
    \begin{align}
        \text{null}(A) = \left\{ \textbf{x} \in {^n\mathbb{R}} \, | \, A\textbf{x} = \textbf{0} \right\}
    \end{align}
    is the null space of $A$, otherwise known as the \textbf{kernel} of $A$ or the solution space of $A\textbf{x} = \textbf{0}$
\end{definition}
    \paragraph{} We can use the Subspace Test I to show that the null space of a matrix is a subspace of $^n\mathbb{R}$.
        \begin{enumerate}
            \item \textbf{Existence of Additive Identity:} \\
            The zero vector is in the null space of $A$ as the trivial solution to the equation $A\textbf{x} = \textbf{0}$
            \item \textbf{Closure Under Addition:} \\
            Let $\textbf{x}, \textbf{y} \in \text{null}(A)$, then $A(\textbf{x} + \textbf{y}) = A\textbf{x} + A\textbf{y} = \textbf{0} + \textbf{0} = \textbf{0}$. Thus, $\textbf{x} + \textbf{y} \in \text{null}(A)$. This holds since $^n\mathbb{R}$ is a vector space.
            \item \textbf{Closure Under Scalar Multiplication:} \\
            Let $\textbf{x} \in \text{null}(A)$ and $\alpha \in \mathbb{R}$, then $A(\alpha \textbf{x}) = \alpha A\textbf{x} = \alpha \textbf{0} = \textbf{0}$. Thus, $\alpha \textbf{x} \in \text{null}(A)$. This holds since $^n\mathbb{R}^m$ is a vector space under usual addition and scalar multiplication.
        \end{enumerate}
\begin{theorem}[Subspace Test, II]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha, \beta \in \mathbb{R}$:
    \begin{enumerate}
        \item $\alpha \textbf{x} + \beta \textbf{y} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the condition is satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Intersection of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cap B = \left\{ x \, | \, x \in A \, \text{and} \, x \in B \right\}
    \end{align}
\end{definition}
\begin{definition}[Union of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cup B = \left\{ x \, | \, x \in A \, \text{or} \, x \in B \right\}
    \end{align}
\end{definition}

\section{Linear combinations}
\begin{definition}[Linear Combination]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n
    \end{align*}
    is a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\subsection{Linear Independence, Span, and Basis}
\begin{definition}[Linear independence]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$. If and only if the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{0}
    \end{align*}
    has \textbf{only} the trivial solution $\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0$.

    Then, $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are said to be linearly independent; otherwise, they are linearly dependent.
\end{definition}
\begin{definition}[Span]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$, then:
    \begin{align}
        \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = \left\{ \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n \, | \, \alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R} \right\}
    \end{align}
    is the set of all linear combinations of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\begin{theorem}[Unique Representation]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}/0$, and that they span $V$. For any $\textbf{v}_{n+1} \in V$, if $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent, then the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{v}_{n+1}  
    \end{align*} 
    is the only way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$.
\end{theorem}
We can prove by contrapositive:
\begin{proof}
    Assume that there exists another way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \beta_1 \textbf{v}_1 + \beta_2 \textbf{v}_2 + \ldots + \beta_n \textbf{v}_n
    \end{align*}
    Rearranging the equation, we have:
    \begin{align*}
        (\alpha_1 - \beta_1) \textbf{v}_1 + (\alpha_2 - \beta_2) \textbf{v}_2 + \ldots + (\alpha_n - \beta_n) \textbf{v}_n = \textbf{0}
    \end{align*}
    Since ($\alpha_i - \beta_i$) for $i = 1, 2, \ldots, n$ are not all zero, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent. Thus, the contrapositive is true.
\end{proof}
\begin{theorem}[Growing and Pruning] The following are the two theorems describe the relationship between linear independence and the span: \\ \\
    \textbf{Growing} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ be linearly independent. If $\textbf{v}_{n+1} \in V$ and $\textbf{v}_{n+1} \notin \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n)$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n, \textbf{v}_{n+1}$ are linearly independent. \\ \\
    \textbf{Pruning} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent, then there exists a $\textbf{v}_i$ such that $\textbf{v}_i \in \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{i-1}, \textbf{v}_{i+1}, \ldots, \textbf{v}_n)$.
\end{theorem}
\begin{theorem}[Span and Linear Independence]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. For every $\textbf{v}_k$ with $k = 1, 2, \ldots, n$, $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1}, \textbf{v}_{k+1}, \ldots, \textbf{v}_n) \subset \text{Span}(\textbf{v}_1, \ldots, \textbf{v}_n)$ if and only if $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent.
\end{theorem}
\begin{definition}[Basis]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ is a basis of $V$.
\end{definition}
\subsection{Fundamental Theorem of Linear Algebra}
\begin{theorem}[Fundamental Theorem of Linear Algebra]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then:
    \begin{enumerate}
        \item Any basis of $V$ has the same number of elements.
        \item Any linearly independent set of $V$ has at most $n$ elements.
        \item Any spanning set of $V$ has at least $n$ elements.
    \end{enumerate}
\end{theorem}
\begin{example}[Dimension of Even 4-degree polynomial]
    Given $W \subseteq \mathbb{P}_4$ where $W = \left\{ p(x) \, | \, p(x) = p(-x) \right\}$, then $W$ is a subspace of $\mathbb{P}_4$. Since $x^3, x \in P_4$ but $x^3, x \notin W$,  Span$(\{W, x, x^3\}) = \mathrm{dim}(W) + 2$ So $\mathrm{dim}(W) \le 3$; We can find 3 linearly independent vectors in $W$ such as $1, x^2, x^4$. Thus, $\mathrm{dim}(W) \ge 3$. Thus, $\mathrm{dim}(W) = 3$.
\end{example}

\begin{definition}[Column Space]
    Let $a_1, a_2, \ldots, {a_n} \in {^m\mathbb{R}}$, then:   
    \begin{equation}
        \text{Col}(A) = \text{Span}({a_1}, {a_2}, \ldots, {a_n})
    \end{equation}
    is the column space of $A$.
\end{definition}
\begin{definition}[Row Space]
    Let $a_1, a_2, \ldots, {a_n} \in {^m\mathbb{R}}$, then:   
    \begin{equation}
        \text{Row}(A) = \text{Span}({a_1}, {a_2}, \ldots, {a_n})
    \end{equation}
    is the row space of $A$.
\end{definition}
Note that the span of set is a subspace of the vector space. Thus, the column space of a matrix is a subspace of $^m\mathbb{R}$. This is also call the set of images of the transformation. We can also note that matrix multiplication is is commutative with scalar multiplication. We can conduct proofs using such properties.
\begin{definition}[Rank]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{equation}
        \text{rank}(A) = \text{dim}(\text{Col}(A))
    \end{equation}
    is the rank of $A$.
\end{definition}
\begin{theorem}[Rank of RREF]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{equation}
        \text{rank}(A) = \text{rank}(\text{RREF}(A))
    \end{equation}
\end{theorem}
\begin{theorem}[Dimension of Row and Column Space]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{align}
        \text{dim}(\text{Row}(A)) &= \text{rank}(A) \\
        \text{dim}(\text{Col}(A)) &= \text{rank}(A)
    \end{align}
\end{theorem}
\begin{definition}[Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        (AB)_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
    \end{equation}
\end{definition}
\begin{definition}[Column View of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        AB = \begin{bmatrix} A\textbf{b}_1 & A\textbf{b}_2 & \ldots & A\textbf{b}_p \end{bmatrix}
    \end{equation}
\end{definition}
\begin{theorem}[Column Space of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        \text{Col}(AB) \subseteq \text{Col}(A)
    \end{equation}
    If $B$ is a invertible matrix, then also $\text{Col}(A) \subseteq \text{Col}(AB)$
\end{theorem}
\begin{definition}[Invertible]
    Let $A \in {^n\mathbb{R}^n}$, then $A$ is invertible if there exists a matrix $B \in {^n\mathbb{R}^n}$ such that:
    \begin{align}
        AB = BA = I
    \end{align}
    where $I$ is the identity matrix, and we denote $B = A^{-1}$, the inverse of $A$.
\end{definition}
\begin{proof}
    Let $A$, $B$ be matrices, so $\text{col}(AB) = \text{Span}(\text{columns of }AB)$. In the column view, we have $AB = A(\text{columns of }B) = \begin{bmatrix}
        A\textbf{b}_1 & A\textbf{b}_2 & \ldots & A\textbf{b}_p
    \end{bmatrix}$
    Each column of $AB$ is in $\text{Col}(A)$. So each column of $AB$ is a linear combination of the columns of $A$. Thus, $\text{Col}(AB) \subseteq \text{Col}(A)$.

    Consider the case where $B$ is invertible. Let $C = AB$, Then $CB^{-1} = A$. Since it is proven that $\text{col}(CB^{-1}) \subseteq \text{col}(C)$, hence,  $\text{col}(A) \subseteq \text{col}(AB)$.
\end{proof}
\begin{definition}[Null Space of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        \text{null}(AB) \supseteq \text{null}(B)
    \end{equation}
\end{definition}
\begin{definition}[Rank-Nullity Theorem]
    Let $A \in \mathbb{R}^{m \times n}$. Then:
    \begin{equation}
        \dim(\text{null}(A)) = n - \text{rank}(A)
    \end{equation}
    where $n$ is the number of columns of $A$.
\end{definition}

\begin{proof}
    Consider $A \in {^m\mathbb{R}^n}$, Let $S = \{x_1, x_2, \ldots, x_k\}$ be a basis of $\text{null}(A)$, then $\{x_1, x_2, \ldots, x_k\}$ are linearly independent and that $\text{dim}(\text{null}(A)) = k$. We can extend $S$ to a basis of $^n\mathbb{R}$ by adding $n-k$ vectors, $\{x_{k+1}, x_{k+2}, \ldots, x_n\}$, then $\{x_1, x_2, \ldots, x_n\}$ is a basis of $^n\mathbb{R}$. Since $\{x_1, x_2, \ldots, x_n\}$ is a basis of $^n\mathbb{R}$, then $\text{rank}(A) = n - k$. Assert hat $n-k \ge 0$. If we can show that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is a basis of $\text{Col}(A)$, then we can show that $\text{rank}(A) = n - k$. We can show that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is a basis of $\text{Col}(A)$ by showing that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is linearly independent and that $\text{Span}(\{Ax_1, Ax_2, \ldots, Ax_n\}) = \text{Col}(A)$.
    
\end{proof}
\begin{theorem}[Properties of Row and Column Space w.r.t RREF]
    Let $A \in {^m\mathbb{R}^n}$ and $\tilde{A} = \text{RREF}(A)$, then:
    \begin{align}
        \text{dim}(\text{Row}(\tilde{A})) &= \text{dim}(\text{Row}(A)) \\
        \text{dim}(\text{Col}(\tilde{A})) &= \text{dim}(\text{Col}(A)) \\
        \text{Row}(\tilde{A}) &= \text{Row}(A) \\
        \text{Col}(\tilde{A}) &\neq \text{Col}(A)
    \end{align}
\end{theorem}
\section{Linear Transformations}
\begin{definition}[Linear Transformation]
    Let $V$ and $W$ be vector spaces, then $T: V \rightarrow W$ is a linear transformation if:
    \begin{enumerate}
        \item $T(\textbf{x} + \textbf{y}) = T(\textbf{x}) + T(\textbf{y})$
        \item $T(\alpha \textbf{x}) = \alpha T(\textbf{x})$
    \end{enumerate}
\end{definition}
\begin{example}[$Ax + b$ is not LT]
    Let $A \in {^m\mathbb{R}^n}$ and $b \in \mathbb{R}^m$, then $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ defined by $T(\textbf{x}) = A\textbf{x} + b$ is not a linear transformation. It violates the first property of linear transformation:
    \begin{align*}
        T(\textbf{x} + \textbf{y}) &= A(\textbf{x} + \textbf{y}) + b \\
        &= A\textbf{x} + A\textbf{y} + b \\
        &= T(\textbf{x}) + T(\textbf{y}) + b
    \end{align*}
\end{example}
\begin{example}[Rotate by $90^\circ$]
    Let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $T(\textbf{x}) = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x}$, then $T$ is a linear transformation. It satisfies the properties of linear transformation.
    \begin{align*}
        T(\textbf{x} + \textbf{y}) &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}(\textbf{x} + \textbf{y}) \\
        &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x} + \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{y} \\
        &= T(\textbf{x}) + T(\textbf{y})
    \end{align*}
    \begin{align*}
        T(\alpha \textbf{x}) &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}(\alpha \textbf{x}) \\
        &= \alpha \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x} \\
        &= \alpha T(\textbf{x})
    \end{align*}
    
\end{example}
\begin{theorem}[Properties of Linear Transformation]
    Let $T: V \rightarrow W$ be a linear transformation, then:
    \begin{enumerate}
        \item $T(\textbf{0}) = \textbf{0}$
        \item $T(\textbf{x} - \textbf{y}) = T(\textbf{x}) - T(\textbf{y})$
    \end{enumerate}
\end{theorem}
\begin{proof}
    We have $T(\textbf{0} + X) = T(X) = T(\textbf{0}) + T(X)$. Thus, $T(\textbf{0}) = \textbf{0}$, by the Cancelation Theorem. We also have $T(\textbf{x} - \textbf{y}) = T(\textbf{x} + (-\textbf{y})) = T(\textbf{x}) + T(-\textbf{y}) = T(\textbf{x}) - T(\textbf{y})$.
\end{proof}
\end{document}