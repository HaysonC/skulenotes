\documentclass[11pt]{article}
\input{preamble}
\begin{document}
\thispagestyle{empty}
{\LARGE \bf MAT 185 Lecture Notes}\\
{\large Hei Shing Cheung}\\
Linear Algebra, Winter 2024 \hfill MAT 185\\
\\
The up-to-date version of this document can be found at \url{https://github.com/HaysonC/skulenotes}\\
\section{Vector Space}
\subsection{Foundamental Properties}
\begin{definition}[Vector Space in $\mathbb{R}$]
    This course concerns with real vector spaces. A vectors space, ${V}$, over $\mathbb{R}$ is a collection of \textbf{object} $\textbf{v} \in V$ s.t. the follow axioms are followed
    \begin{enumerate}
        \item \textbf{Addition Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Addition:} \\
            $\textbf{x}, \textbf{y} \in V \implies \textbf{x} + \textbf{y} \in V$
            
            \item \textbf{Associativity of Addition:} \\
            $\textbf{x}, \textbf{y}, \textbf{z} \in V \implies (\textbf{x} + \textbf{y}) + \textbf{z} = \textbf{x} + (\textbf{y} + \textbf{z})$
            
            \item \textbf{Existence of Additive Identity:} \\
            $\exists \, \textbf{0} \in V$ such that $\textbf{x} \in V \implies \textbf{x} + \textbf{0} = \textbf{x}$
            
            \item \textbf{Existence of Additive Inverse:} \\
            $\forall \, \textbf{x} \in V, \, \exists \, \textbf{-x} \in V$ such that $\textbf{x} + \textbf{-x} = \textbf{0}$
        \end{enumerate}
        
        \item \textbf{Scalar Multiplication Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha \in \mathbb{R}, \, \alpha \textbf{x} \in V$
            
            \item \textbf{Associativity of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha \beta) \textbf{x} = \alpha (\beta \textbf{x})$
            
            \item \textbf{Distributive Property of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha + \beta) \textbf{x} = \alpha \textbf{x} + \beta \textbf{x}$
            
            \item \textbf{Existence of Multiplicative Identity:} \\
            $\forall \, \textbf{x} \in V, \, \textbf{1} \textbf{x} = \textbf{x}$
        \end{enumerate}
    \end{enumerate}
     
    \paragraph{Note} It could be shown that the axiom imply the commutativity of in addition, namely $\forall \textbf{x} \implies \textbf{y} \in \mathbb{R}$, $\textbf{x} + \textbf{y} = \textbf{y} + \textbf{x}$
\end{definition}
\paragraph{Example} $\mathbb{R}^n$ is a vector space over $\mathbb{R}$, where $\textbf{x} = (x_1, x_2, \ldots, x_n)$ and $\textbf{y} = (y_1, y_2, \ldots, y_n)$, then $\textbf{x} + \textbf{y} = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)$ and $\alpha\textbf{x} = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n)$
\begin{theorem}[Cancelation, Part 1]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\ 
        \intertext{From additive inverse we know that \textbf{-z} exists}
        (\textbf{x} + \textbf{z}) + \textbf{-z} &= (\textbf{y} + \textbf{z}) + \textbf{-z} \\
        \intertext{By order of addition we have: }
        \textbf{x} + (\textbf{z} + \textbf{-z}) &= \textbf{y} + (\textbf{z} + \textbf{-z}) \\
        \textbf{x} + \textbf{0} &= \textbf{y} + \textbf{0} \\
        \intertext{By additive identity we have: }
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{theorem}[Cancelation, Part 2]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
To proof that, it would require the following propostion:
\begin{lemma}
    Let $V$ be a vector space and $\textbf{z} \in V$, then $\textbf{-z} + \textbf{z} = 0 $
\end{lemma}
\begin{proof}
    \begin{align*}
        \intertext{We know:}
        \textbf{-z} + \textbf{z} &= (\textbf{-z} + \textbf{z}) + 0\\
        &= (\textbf{-z} + \textbf{z}) + (\textbf{-z} + \textbf{-(-z)}) \\
        \intertext{By the order of additon, }
        &= \textbf{-z} + (\textbf{z} + (\textbf{-z} + \textbf{-(-z)})) \\
        \intertext{Again, by the order of additon, }
        &= \textbf{-z} + ((\textbf{z} + \textbf{-z}) + \textbf{-(-z)}) \\
        &= \textbf{-z} + \textbf{-(-z)} \\
        &= 0
    \end{align*}
\end{proof}
Now, to prove the part 2 of the Cancelation Theorem:
\begin{proof}
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \intertext{From additive inverse we know that \textbf{-z} exists}
        \textbf{-z} + (\textbf{z} + \textbf{x}) &= \textbf{-z} + (\textbf{z} + \textbf{y})\\
        (\textbf{-z} + \textbf{z}) + \textbf{x} &= (\textbf{-z} + \textbf{z}) + \textbf{y}\\
        \intertext{From above, we have $\textbf{-z} + \textbf{z} = 0$}
        0 + \textbf{x} &= 0 + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{lemma}[Inverse of an inverse]
    Let $V$ be a vector space and $\textbf{x} \in V$, then:
    \begin{align*}
        \textbf{-(-x)} &= \textbf{x}
    \end{align*}
\end{lemma}
\begin{proof}
    Assume $0$, $0^\star$ are the additive identity of $V$ and $\textbf{-x}$, $\textbf{-x}^\star$ are the additive inverse of $\textbf{x}$. We have:
    \begin{equation*}
        u+0 = u + 0^\star
    \end{equation*}
    By Cancelation Theorem, we have $0 = 0^\star$. Thus, the additive identity is unique. Now, we have:
    \begin{align*}
        \textbf{x} + \textbf{-x} &= 0 \\
        \textbf{x} + \textbf{-x}^\star &= 0
        \intertext{Thus, by the uniqueness of the additive inverse, we have:}
        \textbf{x} + \textbf{-x} &= \textbf{x} + \textbf{-x}^\star
        \intertext{By the Cancelation Theorem, we have:}
        \textbf{-x} &= \textbf{-x}^\star
    \end{align*}
    Now, we have that the additive inverse is unique. Thus, $\textbf{-(-x)}$ must be unique and nessarily $\textbf{x}$. 
\end{proof}
\paragraph{Additional Results from Above}
\begin{enumerate}
    \item The additive identity is unique
    \item The additive inverse is unique
\end{enumerate}
\begin{definition}[Subtraction]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} - \textbf{y} &\defeq \textbf{x} + \textbf{-y}
    \end{align*}
\end{definition}
\begin{theorem}[Addition is Commutative]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{y} &= \textbf{y} + \textbf{x}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{y} &=  
    \end{align*}
\end{proof} 
\subsection{Vector Subspace}
\begin{definition}[Vector Subspace]
    Let $V$ be a vector space and $W \subseteq V$, then $W$ is a vector subspace of $V$ if $W$ is a vector space.
\end{definition}
\begin{theorem}[Subspace Test, I]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha \in \mathbb{R}$:
    \begin{enumerate}
        \item \textbf{Closure Under Addition:} \\
        $\textbf{x} + \textbf{y} \in W$
        \item \textbf{Closure Under Scalar Multiplication:} \\
        $\alpha \textbf{x} \in W$
        \item \text{Additive Identity:} \\
        $\textbf{0} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the three conditions are satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Null Space]
    Let $V$ be a vector space and $A \in {^m\mathbb{R}^n}$\footnote{Same as $\mathbb{R}^{m\times n}$, The set of all $m \times n$ matrices with real entries}, then:
    \begin{align}
        \text{null}(A) = \left\{ \textbf{x} \in {^n\mathbb{R}} \, | \, A\textbf{x} = \textbf{0} \right\}
    \end{align}
    is the null space of $A$, otherwise known as the \textbf{kernel} of $A$ or the solution space of $A\textbf{x} = \textbf{0}$
\end{definition}
    \paragraph{} We can use the Subspace Test I to show that the null space of a matrix is a subspace of $^n\mathbb{R}$.
        \begin{enumerate}
            \item \textbf{Existence of Additive Identity:} \\
            The zero vector is in the null space of $A$ as the trivial solution to the equation $A\textbf{x} = \textbf{0}$
            \item \textbf{Closure Under Addition:} \\
            Let $\textbf{x}, \textbf{y} \in \text{null}(A)$, then $A(\textbf{x} + \textbf{y}) = A\textbf{x} + A\textbf{y} = \textbf{0} + \textbf{0} = \textbf{0}$. Thus, $\textbf{x} + \textbf{y} \in \text{null}(A)$. This holds since $^n\mathbb{R}$ is a vector space.
            \item \textbf{Closure Under Scalar Multiplication:} \\
            Let $\textbf{x} \in \text{null}(A)$ and $\alpha \in \mathbb{R}$, then $A(\alpha \textbf{x}) = \alpha A\textbf{x} = \alpha \textbf{0} = \textbf{0}$. Thus, $\alpha \textbf{x} \in \text{null}(A)$. This holds since $^n\mathbb{R}^m$ is a vector space under usual addition and scalar multiplication.
        \end{enumerate}
\begin{theorem}[Subspace Test, II]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha, \beta \in \mathbb{R}$:
    \begin{enumerate}
        \item $\alpha \textbf{x} + \beta \textbf{y} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the condition is satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Intersection of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cap B = \left\{ x \, | \, x \in A \, \text{and} \, x \in B \right\}
    \end{align}
\end{definition}
\begin{definition}[Union of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cup B = \left\{ x \, | \, x \in A \, \text{or} \, x \in B \right\}
    \end{align}
\end{definition}

\section{Linear combinations}
\begin{definition}[Linear Combination]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n
    \end{align*}
    is a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\subsection{Linear Independence, Span, and Basis}
\begin{definition}[Linear independence]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$. If and only if the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{0}
    \end{align*}
    has \textbf{only} the trivial solution $\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0$.

    Then, $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are said to be linearly independent; otherwise, they are linearly dependent.
\end{definition}
\begin{definition}[Span]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$, then:
    \begin{align}
        \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = \left\{ \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n \, | \, \alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R} \right\}
    \end{align}
    is the set of all linear combinations of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\begin{theorem}[Unique Representation]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}/0$, and that they span $V$. For any $\textbf{v}_{n+1} \in V$, if $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent, then the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{v}_{n+1}  
    \end{align*} 
    is the only way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$.
\end{theorem}
We can prove by contrapositive:
\begin{proof}
    Assume that there exists another way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \beta_1 \textbf{v}_1 + \beta_2 \textbf{v}_2 + \ldots + \beta_n \textbf{v}_n
    \end{align*}
    Rearranging the equation, we have:
    \begin{align*}
        (\alpha_1 - \beta_1) \textbf{v}_1 + (\alpha_2 - \beta_2) \textbf{v}_2 + \ldots + (\alpha_n - \beta_n) \textbf{v}_n = \textbf{0}
    \end{align*}
    Since ($\alpha_i - \beta_i$) for $i = 1, 2, \ldots, n$ are not all zero, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent. Thus, the contrapositive is true.
\end{proof}
\begin{theorem}[Growing and Pruning] The following are the two theorems describe the relationship between linear independence and the span: \\ \\
    \textbf{Growing} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ be linearly independent. If $\textbf{v}_{n+1} \in V$ and $\textbf{v}_{n+1} \notin \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n)$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n, \textbf{v}_{n+1}$ are linearly independent. \\ \\
    \textbf{Pruning} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent, then there exists a $\textbf{v}_i$ such that $\textbf{v}_i \in \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{i-1}, \textbf{v}_{i+1}, \ldots, \textbf{v}_n)$.
\end{theorem}
\begin{definition}[Basis]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ is a basis of $V$.
\end{definition}
\subsection{Fundamental Theorem of Linear Algebra}
\begin{theorem}[Fundamental Theorem of Linear Algebra]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then:
    \begin{enumerate}
        \item Any basis of $V$ has the same number of elements.
        \item Any linearly independent set of $V$ has at most $n$ elements.
        \item Any spanning set of $V$ has at least $n$ elements.
    \end{enumerate}
\end{theorem}
\end{document}