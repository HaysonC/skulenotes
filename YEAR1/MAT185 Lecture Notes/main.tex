\documentclass[11pt]{report}
\input{preamble}
\begin{document}
\thispagestyle{empty}
{\LARGE \bf MAT 185 Lecture Notes}\\
{\large Hei Shing Cheung}\\
Linear Algebra, Winter 2024 \hfill MAT 185\\
\\
The up-to-date version of this document can be found at \url{https://github.com/HaysonC/skulenotes}\\
\section{Vector Space}
\subsection{Foundamental Properties}
\begin{definition}[Vector Space in $\mathbb{R}$, Field]
    This course concerns with real vector spaces. A vectors space, ${V}$, over $\mathbb{R}$ is a collection of \textbf{object} $\textbf{v} \in V$ s.t. the follow axioms are followed
    \begin{enumerate}
        \item \textbf{Addition Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Addition:} \\
            $\textbf{x}, \textbf{y} \in V \implies \textbf{x} + \textbf{y} \in V$
            
            \item \textbf{Associativity of Addition:} \\
            $\textbf{x}, \textbf{y}, \textbf{z} \in V \implies (\textbf{x} + \textbf{y}) + \textbf{z} = \textbf{x} + (\textbf{y} + \textbf{z})$
            
            \item \textbf{Existence of Additive Identity:} \\
            $\exists \, \textbf{0} \in V$ such that $\textbf{x} \in V \implies \textbf{x} + \textbf{0} = \textbf{x}$
            
            \item \textbf{Existence of Additive Inverse:} \\
            $\forall \, \textbf{x} \in V, \, \exists \, \textbf{-x} \in V$ such that $\textbf{x} + \textbf{-x} = \textbf{0}$
        \end{enumerate}
        
        \item \textbf{Scalar Multiplication Axioms}
        \begin{enumerate}
            \item \textbf{Closure Under Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha \in \mathbb{R}, \, \alpha \textbf{x} \in V$
            
            \item \textbf{Associativity of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha \beta) \textbf{x} = \alpha (\beta \textbf{x})$
            
            \item \textbf{Distributive Property of Scalar Multiplication:} \\
            $\forall \, \textbf{x} \in V \, \text{and} \, \forall \, \alpha, \beta \in \mathbb{R}, \, (\alpha + \beta) \textbf{x} = \alpha \textbf{x} + \beta \textbf{x}$
            
            \item \textbf{Existence of Multiplicative Identity:} \\
            $\forall \, \textbf{x} \in V, \, \textbf{1} \textbf{x} = \textbf{x}$
        \end{enumerate}
    \end{enumerate}
     
    \paragraph{Note} It could be shown that the axiom imply the commutativity of in addition, namely $\forall \textbf{x}, \textbf{y} \in \mathbb{R}$, $\textbf{x} + \textbf{y} = \textbf{y} + \textbf{x}$
\end{definition}
\paragraph{Example} $\mathbb{R}^n$ is a vector space over $\mathbb{R}$, where $\textbf{x} = (x_1, x_2, \ldots, x_n)$ and $\textbf{y} = (y_1, y_2, \ldots, y_n)$, then $\textbf{x} + \textbf{y} = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)$ and $\alpha\textbf{x} = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n)$
\begin{theorem}[Cancelation, Part 1]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{z} &= \textbf{y} + \textbf{z} \\ 
        \intertext{From additive inverse we know that \textbf{-z} exists}
        (\textbf{x} + \textbf{z}) + \textbf{-z} &= (\textbf{y} + \textbf{z}) + \textbf{-z} \\
        \intertext{By order of addition we have: }
        \textbf{x} + (\textbf{z} + \textbf{-z}) &= \textbf{y} + (\textbf{z} + \textbf{-z}) \\
        \textbf{x} + \textbf{0} &= \textbf{y} + \textbf{0} \\
        \intertext{By additive identity we have: }
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{theorem}[Cancelation, Part 2]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y}, \textbf{z} \in V$, then:
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*} 
\end{theorem}
To proof that, it would require the following propostion:
\begin{lemma}
    Let $V$ be a vector space and $\textbf{z} \in V$, then $\textbf{-z} + \textbf{z} = 0 $
\end{lemma}
\begin{proof}
    \begin{align*}
        \intertext{We know:}
        \textbf{-z} + \textbf{z} &= (\textbf{-z} + \textbf{z}) + 0\\
        &= (\textbf{-z} + \textbf{z}) + (\textbf{-z} + \textbf{-(-z)}) \\
        \intertext{By the order of additon, }
        &= \textbf{-z} + (\textbf{z} + (\textbf{-z} + \textbf{-(-z)})) \\
        \intertext{Again, by the order of additon, }
        &= \textbf{-z} + ((\textbf{z} + \textbf{-z}) + \textbf{-(-z)}) \\
        &= \textbf{-z} + \textbf{-(-z)} \\
        &= 0
    \end{align*}
\end{proof}
Now, to prove the part 2 of the Cancelation Theorem:
\begin{proof}
    \begin{align*}
        \textbf{z} + \textbf{x} &= \textbf{z} + \textbf{y} \\
        \intertext{From additive inverse we know that \textbf{-z} exists}
        \textbf{-z} + (\textbf{z} + \textbf{x}) &= \textbf{-z} + (\textbf{z} + \textbf{y})\\
        (\textbf{-z} + \textbf{z}) + \textbf{x} &= (\textbf{-z} + \textbf{z}) + \textbf{y}\\
        \intertext{From above, we have $\textbf{-z} + \textbf{z} = 0$}
        0 + \textbf{x} &= 0 + \textbf{y} \\
        \textbf{x} &= \textbf{y}
    \end{align*}
\end{proof}
\begin{lemma}[Inverse of an inverse]
    Let $V$ be a vector space and $\textbf{x} \in V$, then:
    \begin{align*}
        \textbf{-(-x)} &= \textbf{x}
    \end{align*}
\end{lemma}
\begin{proof}
    Assume $0$, $0^\star$ are the additive identity of $V$ and $\textbf{-x}$, $\textbf{-x}^\star$ are the additive inverse of $\textbf{x}$. We have:
    \begin{equation*}
        u+0 = u + 0^\star
    \end{equation*}
    By Cancelation Theorem, we have $0 = 0^\star$. Thus, the additive identity is unique. Now, we have:
    \begin{align*}
        \textbf{x} + \textbf{-x} &= 0 \\
        \textbf{x} + \textbf{-x}^\star &= 0
        \intertext{Thus, by the uniqueness of the additive inverse, we have:}
        \textbf{x} + \textbf{-x} &= \textbf{x} + \textbf{-x}^\star
        \intertext{By the Cancelation Theorem, we have:}
        \textbf{-x} &= \textbf{-x}^\star
    \end{align*}
    Now, we have that the additive inverse is unique. Thus, $\textbf{-(-x)}$ must be unique and nessarily $\textbf{x}$. 
\end{proof}
\paragraph{Additional Results from Above}
\begin{enumerate}
    \item The additive identity is unique
    \item The additive inverse is unique
\end{enumerate}
\begin{definition}[Subtraction]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} - \textbf{y} &\defeq \textbf{x} + \textbf{-y}
    \end{align*}
\end{definition}
\begin{theorem}[Addition is Commutative]
    Let $V$ be a vector space and $\textbf{x}, \textbf{y} \in V$, then:
    \begin{align*}
        \textbf{x} + \textbf{y} &= \textbf{y} + \textbf{x}
    \end{align*} 
\end{theorem}
\begin{proof}
    \begin{align*}
        \textbf{x} + \textbf{y} &=  
    \end{align*}
\end{proof} 
\subsection{Vector Subspace}
\begin{definition}[Vector Subspace]
    Let $V$ be a vector space and $W \subseteq V$, then $W$ is a vector subspace of $V$ if $W$ is a vector space.
\end{definition}
\begin{theorem}[Subspace Test, I]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha \in \mathbb{R}$:
    \begin{enumerate}
        \item \textbf{Closure Under Addition:} \\
        $\textbf{x} + \textbf{y} \in W$
        \item \textbf{Closure Under Scalar Multiplication:} \\
        $\alpha \textbf{x} \in W$
        \item \text{Additive Identity:} \\
        $\textbf{0} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the three conditions are satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Null Space]
    Let $V$ be a vector space and $A \in {^m\mathbb{R}^n}$\footnote{Same as $\mathbb{R}^{m\times n}$, The set of all $m \times n$ matrices with real entries}, then:
    \begin{align}
        \text{null}(A) = \left\{ \textbf{x} \in {^n\mathbb{R}} \, | \, A\textbf{x} = \textbf{0} \right\}
    \end{align}
    is the null space of $A$, otherwise known as the \textbf{kernel} of $A$ or the solution space of $A\textbf{x} = \textbf{0}$
\end{definition}
    \paragraph{} We can use the Subspace Test I to show that the null space of a matrix is a subspace of $^n\mathbb{R}$.
        \begin{enumerate}
            \item \textbf{Existence of Additive Identity:} \\
            The zero vector is in the null space of $A$ as the trivial solution to the equation $A\textbf{x} = \textbf{0}$
            \item \textbf{Closure Under Addition:} \\
            Let $\textbf{x}, \textbf{y} \in \text{null}(A)$, then $A(\textbf{x} + \textbf{y}) = A\textbf{x} + A\textbf{y} = \textbf{0} + \textbf{0} = \textbf{0}$. Thus, $\textbf{x} + \textbf{y} \in \text{null}(A)$. This holds since $^n\mathbb{R}$ is a vector space.
            \item \textbf{Closure Under Scalar Multiplication:} \\
            Let $\textbf{x} \in \text{null}(A)$ and $\alpha \in \mathbb{R}$, then $A(\alpha \textbf{x}) = \alpha A\textbf{x} = \alpha \textbf{0} = \textbf{0}$. Thus, $\alpha \textbf{x} \in \text{null}(A)$. This holds since $^n\mathbb{R}^m$ is a vector space under usual addition and scalar multiplication.
        \end{enumerate}
\begin{theorem}[Subspace Test, II]
    Let $V$ be a vector space over $\mathbb{R}$ and $W \subseteq V$ with the usual addition and scalar multiplication, then $W$ is a subspace of $V$ iff for all $\textbf{x}, \textbf{y} \in W$ and $\alpha, \beta \in \mathbb{R}$:
    \begin{enumerate}
        \item $\alpha \textbf{x} + \beta \textbf{y} \in W$
    \end{enumerate}
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Assume $W$ is a subspace of $V$, then $W$ is a vector space. Thus, the axioms of vector space are satisfied. \\
    ($\Leftarrow$) Assume the condition is satisfied, then $W$ is a vector space. Thus, $W$ is a subspace of $V$.
\end{proof}
\begin{definition}[Intersection of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cap B = \left\{ x \, | \, x \in A \, \text{and} \, x \in B \right\}
    \end{align}
\end{definition}
\begin{definition}[Union of Sets]
    Let $A$ and $B$ be sets, then:
    \begin{align}
        A \cup B = \left\{ x \, | \, x \in A \, \text{or} \, x \in B \right\}
    \end{align}
\end{definition}

\section{Linear combinations and Bases}
\begin{definition}[Linear Combination]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n
    \end{align*}
    is a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\subsection{Linear Independence, Span, and Basis}
\begin{definition}[Linear independence]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}$. If and only if the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{0}
    \end{align*}
    has \textbf{only} the trivial solution $\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0$.

    Then, $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are said to be linearly independent; otherwise, they are linearly dependent.
\end{definition}
\begin{definition}[Span]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$, then:
    \begin{align}
        \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = \left\{ \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n \, | \, \alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R} \right\}
    \end{align}
    is the set of all linear combinations of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$
\end{definition}
\begin{theorem}[Unique Representation]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ and $\alpha_1, \alpha_2, \ldots, \alpha_n \in \mathbb{R}/0$, and that they span $V$. For any $\textbf{v}_{n+1} \in V$, if $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent, then the equation:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \textbf{v}_{n+1}  
    \end{align*} 
    is the only way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$.
\end{theorem}
We can prove by contrapositive:
\begin{proof}
    Assume that there exists another way to express $\textbf{v}_{n+1}$ as a linear combination of $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$, then:
    \begin{align*}
        \alpha_1 \textbf{v}_1 + \alpha_2 \textbf{v}_2 + \ldots + \alpha_n \textbf{v}_n = \beta_1 \textbf{v}_1 + \beta_2 \textbf{v}_2 + \ldots + \beta_n \textbf{v}_n
    \end{align*}
    Rearranging the equation, we have:
    \begin{align*}
        (\alpha_1 - \beta_1) \textbf{v}_1 + (\alpha_2 - \beta_2) \textbf{v}_2 + \ldots + (\alpha_n - \beta_n) \textbf{v}_n = \textbf{0}
    \end{align*}
    Since ($\alpha_i - \beta_i$) for $i = 1, 2, \ldots, n$ are not all zero, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent. Thus, the contrapositive is true.
\end{proof}
\begin{theorem}[Growing and Pruning] The following are the two theorems describe the relationship between linear independence and the span: \\ \\
    \textbf{Growing} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$ be linearly independent. If $\textbf{v}_{n+1} \in V$ and $\textbf{v}_{n+1} \notin \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n)$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n, \textbf{v}_{n+1}$ are linearly independent. \\ \\
    \textbf{Pruning} Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly dependent, then there exists a $\textbf{v}_i$ such that $\textbf{v}_i \in \text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{i-1}, \textbf{v}_{i+1}, \ldots, \textbf{v}_n)$.
\end{theorem}
\begin{theorem}[Span and Linear Independence]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. For every $\textbf{v}_k$ with $k = 1, 2, \ldots, n$, $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1}, \textbf{v}_{k+1}, \ldots, \textbf{v}_n) \subset \text{Span}(\textbf{v}_1, \ldots, \textbf{v}_n)$ if and only if $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent.
\end{theorem}
\begin{definition}[Basis]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ is a basis of $V$.
\end{definition}
\subsection{Fundamental Theorem of Linear Algebra}
\begin{theorem}[Fundamental Theorem of Linear Algebra]
    Let $V$ be a vector space and $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n \in V$. If $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n$ are linearly independent and $\text{Span}(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n) = V$, then:
    \begin{enumerate}
        \item Any basis of $V$ has the same number of elements.
        \item Any linearly independent set of $V$ has at most $n$ elements.
        \item Any spanning set of $V$ has at least $n$ elements.
    \end{enumerate}
\end{theorem}
\begin{example}[Dimension of Even 4-degree polynomial]
    Given $W \subseteq \mathbb{P}_4$ where $W = \left\{ p(x) \, | \, p(x) = p(-x) \right\}$, then $W$ is a subspace of $\mathbb{P}_4$. Since $x^3, x \in P_4$ but $x^3, x \notin W$,  Span$(\{W, x, x^3\}) = \mathrm{dim}(W) + 2$ So $\mathrm{dim}(W) \le 3$; We can find 3 linearly independent vectors in $W$ such as $1, x^2, x^4$. Thus, $\mathrm{dim}(W) \ge 3$. Thus, $\mathrm{dim}(W) = 3$.
\end{example}

\begin{definition}[Column Space]
    Let $a_1, a_2, \ldots, {a_n} \in {^m\mathbb{R}}$, then:   
    \begin{equation}
        \text{Col}(A) = \text{Span}({a_1}, {a_2}, \ldots, {a_n})
    \end{equation}
    is the column space of $A$.
\end{definition}
\begin{definition}[Row Space]
    Let $a_1, a_2, \ldots, {a_n} \in {^m\mathbb{R}}$, then:   
    \begin{equation}
        \text{Row}(A) = \text{Span}({a_1}, {a_2}, \ldots, {a_n})
    \end{equation}
    is the row space of $A$.
\end{definition}
Note that the span of set is a subspace of the vector space. Thus, the column space of a matrix is a subspace of $^m\mathbb{R}$. This is also call the set of images of the transformation. We can also note that matrix multiplication is is commutative with scalar multiplication. We can conduct proofs using such properties.
\begin{definition}[Rank]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{equation}
        \text{rank}(A) = \text{dim}(\text{Col}(A))
    \end{equation}
    is the rank of $A$.
\end{definition}
\begin{theorem}[Rank of RREF]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{equation}
        \text{rank}(A) = \text{rank}(\text{RREF}(A))
    \end{equation}
\end{theorem}
\begin{theorem}[Dimension of Row and Column Space]
    Let $A \in {^m\mathbb{R}^n}$, then:
    \begin{align}
        \text{dim}(\text{Row}(A)) &= \text{rank}(A) \\
        \text{dim}(\text{Col}(A)) &= \text{rank}(A)
    \end{align}
\end{theorem}
\begin{definition}[Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        (AB)_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
    \end{equation}
\end{definition}
\begin{definition}[Column View of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        AB = \begin{bmatrix} A\textbf{b}_1 & A\textbf{b}_2 & \ldots & A\textbf{b}_p \end{bmatrix}
    \end{equation}
\end{definition}
\begin{theorem}[Column Space of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        \text{Col}(AB) \subseteq \text{Col}(A)
    \end{equation}
    If $B$ is a invertible matrix, then also $\text{Col}(A) \subseteq \text{Col}(AB)$
\end{theorem}
\begin{definition}[Invertible]
    Let $A \in {^n\mathbb{R}^n}$, then $A$ is invertible if there exists a matrix $B \in {^n\mathbb{R}^n}$ such that:
    \begin{align}
        AB = BA = I
    \end{align}
    where $I$ is the identity matrix, and we denote $B = A^{-1}$, the inverse of $A$.
\end{definition}
\begin{proof}
    Let $A$, $B$ be matrices, so $\text{col}(AB) = \text{Span}(\text{columns of }AB)$. In the column view, we have $AB = A(\text{columns of }B) = \begin{bmatrix}
        A\textbf{b}_1 & A\textbf{b}_2 & \ldots & A\textbf{b}_p
    \end{bmatrix}$
    Each column of $AB$ is in $\text{Col}(A)$. So each column of $AB$ is a linear combination of the columns of $A$. Thus, $\text{Col}(AB) \subseteq \text{Col}(A)$.

    Consider the case where $B$ is invertible. Let $C = AB$, Then $CB^{-1} = A$. Since it is proven that $\text{col}(CB^{-1}) \subseteq \text{col}(C)$, hence,  $\text{col}(A) \subseteq \text{col}(AB)$.
\end{proof}
\begin{definition}[Null Space of Matrix Multiplication]
    Let $A \in {^m\mathbb{R}^n}$ and $B \in {^n\mathbb{R}^p}$, then:
    \begin{equation}
        \text{null}(AB) \supseteq \text{null}(B)
    \end{equation}
\end{definition}
\begin{definition}[Rank-Nullity Theorem]
    Let $A \in \mathbb{R}^{m \times n}$. Then:
    \begin{equation}
        \dim(\text{null}(A)) = n - \text{rank}(A)
    \end{equation}
    where $n$ is the number of columns of $A$.
\end{definition}

\begin{proof}
    Consider $A \in {^m\mathbb{R}^n}$, Let $S = \{x_1, x_2, \ldots, x_k\}$ be a basis of $\text{null}(A)$, then $\{x_1, x_2, \ldots, x_k\}$ are linearly independent and that $\text{dim}(\text{null}(A)) = k$. We can extend $S$ to a basis of $^n\mathbb{R}$ by adding $n-k$ vectors, $\{x_{k+1}, x_{k+2}, \ldots, x_n\}$, then $\{x_1, x_2, \ldots, x_n\}$ is a basis of $^n\mathbb{R}$. Since $\{x_1, x_2, \ldots, x_n\}$ is a basis of $^n\mathbb{R}$, then $\text{rank}(A) = n - k$. Assert hat $n-k \ge 0$. If we can show that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is a basis of $\text{Col}(A)$, then we can show that $\text{rank}(A) = n - k$. We can show that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is a basis of $\text{Col}(A)$ by showing that $\{Ax_1, Ax_2, \ldots, Ax_n\}$ is linearly independent and that $\text{Span}(\{Ax_1, Ax_2, \ldots, Ax_n\}) = \text{Col}(A)$.
    
\end{proof}
\begin{theorem}[Properties of Row and Column Space w.r.t RREF]
    Let $A \in {^m\mathbb{R}^n}$ and $\tilde{A} = \text{RREF}(A)$, then:
    \begin{align}
        \text{dim}(\text{Row}(\tilde{A})) &= \text{dim}(\text{Row}(A)) \\
        \text{dim}(\text{Col}(\tilde{A})) &= \text{dim}(\text{Col}(A)) \\
        \text{Row}(\tilde{A}) &= \text{Row}(A) \\
        \text{Col}(\tilde{A}) &\neq \text{Col}(A)
    \end{align}
\end{theorem}
\section{Linear Transformations}
\begin{definition}[Linear Transformation]
    Let $V$ and $W$ be vector spaces, then $T: V \rightarrow W$ is a linear transformation if:
    \begin{enumerate}
        \item $T(\textbf{x} + \textbf{y}) = T(\textbf{x}) + T(\textbf{y})$
        \item $T(\alpha \textbf{x}) = \alpha T(\textbf{x})$
    \end{enumerate}
\end{definition}
\begin{example}[$Ax + b$ is not LT]
    Let $A \in {^m\mathbb{R}^n}$ and $b \in \mathbb{R}^m$, then $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ defined by $T(\textbf{x}) = A\textbf{x} + b$ is not a linear transformation. It violates the first property of linear transformation:
    \begin{align*}
        T(\textbf{x} + \textbf{y}) &= A(\textbf{x} + \textbf{y}) + b \\
        &= A\textbf{x} + A\textbf{y} + b \\
        &= T(\textbf{x}) + T(\textbf{y}) + b
    \end{align*}
\end{example}
\begin{example}[Rotate by $90^\circ$]
    Let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $T(\textbf{x}) = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x}$, then $T$ is a linear transformation. It satisfies the properties of linear transformation.
    \begin{align*}
        T(\textbf{x} + \textbf{y}) &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}(\textbf{x} + \textbf{y}) \\
        &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x} + \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{y} \\
        &= T(\textbf{x}) + T(\textbf{y})
    \end{align*}
    \begin{align*}
        T(\alpha \textbf{x}) &= \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}(\alpha \textbf{x}) \\
        &= \alpha \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\textbf{x} \\
        &= \alpha T(\textbf{x})
    \end{align*}
    
\end{example}
\begin{theorem}[Properties of Linear Transformation]
    Let $T: V \rightarrow W$ be a linear transformation, then:
    \begin{enumerate}
        \item $T(\textbf{0}) = \textbf{0}$
        \item $T(\textbf{x} - \textbf{y}) = T(\textbf{x}) - T(\textbf{y})$
    \end{enumerate}
\end{theorem}
\begin{proof}
    We have $T(\textbf{0} + X) = T(X) = T(\textbf{0}) + T(X)$. Thus, $T(\textbf{0}) = \textbf{0}$, by the Cancelation Theorem. We also have $T(\textbf{x} - \textbf{y}) = T(\textbf{x} + (-\textbf{y})) = T(\textbf{x}) + T(-\textbf{y}) = T(\textbf{x}) - T(\textbf{y})$.
\end{proof}
\begin{definition}[Kernel and Image]
    Let $T: V \rightarrow W$ be a linear transformation, then: 
    \begin{enumerate}
        \item The \textbf{kernel} of $T$, denoted as $\ker(T)$, is the set of all vectors in $V$ that map to the zero vector in $W$. Formally,
        \[
        \ker(T) = \{ \textbf{v} \in V \mid T(\textbf{v}) = \textbf{0} \}
        \]
        \item The \textbf{image} of $T$, denoted as $\text{Im}(T)$, is the set of all vectors in $W$ that are images of vectors in $V$. Formally,
        \[
        \text{Im}(T) = \{ T(\textbf{v}) \mid \textbf{v} \in V \}
        \]
    \end{enumerate}
\end{definition}
\begin{theorem}[Rank-Nullity for LT]
    Let $T: V \rightarrow W$ be a linear transformation, then:
    \begin{equation}
        \dim(\ker(T)) + \dim(\text{Im}(T)) = \dim(V)
    \end{equation}
\end{theorem}
\begin{definition}[Injective LT]
    Let $T: V \rightarrow W$ be a linear transformation, then $T$ is injective if $x_1 \neq x_2 \implies T(x_1) \implies T(x_2)$
\end{definition}
\begin{theorem}
    If the nullity of $T$, $\text{dim ker} T$ is greater than zero, then $T$ is not injective.
\end{theorem}
\begin{definition}[Surjective]
    Let $T: V \rightarrow W$ be a linear transformation, then $T$ is surjective if for every $y \in W$, there exist an $x \in V$ so that $T(x) = y$.
\end{definition}
\begin{definition}[Bijective]
    Let $T: V \rightarrow W$ be a linear transformation, then $T$ is bijective if $T$ is both injective and surjective.
\end{definition}
\begin{theorem}[Basis function is LT]
    Let $\alpha = \{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n\}$ be a basis for vector space $V$. The function $T: V \to \mathbb{R}^n$ defined by $T(v) = [v]_\alpha$ is a linear transformation.
\end{theorem}
\subsection{Change of Basis}
\begin{definition}[Change of Basis]
    Let $\alpha = \{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n\}$ and $\beta = \{\textbf{w}_1, \textbf{w}_2, \ldots, \textbf{w}_n\}$ be two bases for vector space $V$. The change of basis matrix $P$ is defined as:
    \begin{equation}
        P = [\textbf{w}_1, \textbf{w}_2, \ldots, \textbf{w}_n]_\alpha
    \end{equation}
\end{definition}
\begin{theorem}[Change of Basis Matrix]
    Let $\alpha = \{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_n\}$ and $\beta = \{\textbf{w}_1, \textbf{w}_2, \ldots, \textbf{w}_n\}$ be two bases for vector space $V$. The change of basis matrix $P$ is defined as:
    \begin{equation}
        P = [\textbf{w}_1, \textbf{w}_2, \ldots, \textbf{w}_n]_\alpha
    \end{equation}
\end{theorem}
\paragraph{Stacking Chnage of Basis Matrix} We can stack the change of basis matrix to form a matrix $P$, and also incoperate linear transformations in other bases to a vector.
\section{Determinants and Inverses}
\subsection{Determinants}
\begin{definition}[Determinant]
    Let $A \in \mathbb{R}^{n \times n}$, then the determinant of $A$ is denoted as $\det(A)$.
\end{definition}
\begin{example}
    Consider the following $2\times 2$ system:
    \begin{align*}
        ax + by &= 0 \\
        cx + dy &= 0
    \end{align*}
    The the system has the general solution:
    \begin{align*}
        x &= \frac{dy}{ad - bc} \\
        y &= \frac{-cx}{ad - bc}
    \end{align*}
    The denominator $ad - bc$ is the determinant of the matrix. Notice that if the determinant is zero, then the system has no unique solution.
\end{example}
\begin{theorem}[Propeties of $2 \times 2$ Determinant]
    Let $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, then:
    \begin{align}
        \det(A) &= ad - bc \\
        \det\left(\begin{bmatrix}
            br_1 + cr_1^\prime \\ r_2
        \end{bmatrix}\right) &= b\det\left(\begin{bmatrix}
            r_1 \\ r_2
        \end{bmatrix}\right) + c\det\left(\begin{bmatrix}
            r_1^\prime \\ r_2 
        \end{bmatrix}\right) \\
        \det\left(\begin{bmatrix}
            r_1 \\ r_2
        \end{bmatrix}\right) &= -\det\left(\begin{bmatrix}
            r_2 \\ r_1
        \end{bmatrix}\right) \\ 
        \det I &= 1
\end{align}
\end{theorem}
\begin{definition}[Determinant Functions]
    Any function $f$ that satisfies the properties:
    \begin{enumerate}
        \item $f\left(\begin{bmatrix}
            br_1 + cr_1^\prime \\ r_2 \\ \vdots \\ r_n
        \end{bmatrix}\right) = bf\left(\begin{bmatrix}
            r_1 \\ r_2 \\ \vdots \\ r_n
        \end{bmatrix}\right) + cf\left(\begin{bmatrix}
            r_1^\prime \\ r_2 \\ \vdots \\ r_n
        \end{bmatrix}\right)$ (Multilinearity in rows)
        
        \item $f\left(\begin{bmatrix}
            r_1 \\ r_2 \\ \vdots \\ r_i \\ \vdots \\ r_j \\ \vdots \\ r_n
        \end{bmatrix}\right) = -f\left(\begin{bmatrix}
            r_1 \\ r_2 \\ \vdots \\ r_j \\ \vdots \\ r_i \\ \vdots \\ r_n
        \end{bmatrix}\right)$ (Alternating property)
        
        \item $f(A) = (\det A )f(I)$
    \end{enumerate}
    is called a determinant function.
\end{definition}
\begin{theorem}
    If $f$ is alternating on rows and $A$ is a matrix that has two identical rows, then $f(A) = 0$.
\end{theorem}
\subsection{Constructing Determinants and Inverses}
\begin{definition}[Minor Matrix]
    Let $A \in {^n\mathbb{R}^n}$, then the minor of $A$ is denoted as $A_{ij}$, which the matrix obtained by removing the $i$-th row and $j$-th column of $A$.
\end{definition}
\begin{example}
    Let $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$, then the minor of $A$ is:
    $$ A_{11} = \begin{bmatrix} 5 & 6 \\ 8 & 9 \end{bmatrix} \quad A_{12} = \begin{bmatrix} 4 & 6 \\ 7 & 9 \end{bmatrix} \quad A_{13} = \begin{bmatrix} 4 & 5 \\ 7 & 8 \end{bmatrix} $$
\end{example}
\paragraph{Find the Determinant from a Row} We can find the determinant of a matrix by expanding along a row. We can expand along any row or column, but it is easier to expand along a row with zeros.
\begin{definition}[Cofactor]
    Let $A \in  {^n\mathbb{R}^n}$, then the cofactor of $A$ is denoted as $C_{ij}$, which is defined as:
    \begin{equation}
        C_{ij} = (-1)^{i+j}\det(A_{ij})
    \end{equation}
\end{definition}
\paragraph{Checkerboard Pattern} We can use the checkerboard pattern to find the determinant of a matrix. We can expand along any row or column, but it is easier to expand along a row with zeros.
\begin{definition}[Determinant]
    Let $A \in  {^n\mathbb{R}^n}$, then the determinant of $A$ is denoted as $\det(A)$, which is defined as:
    \begin{equation}
        \det(A) = \sum_{j=1}^{n} a_{ij}C_{ij}
    \end{equation}
\end{definition}
\begin{example}
    Let $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$, then the cofactor of $A$ is:
    $$ C_{11} = \begin{bmatrix} 5 & 6 \\ 8 & 9 \end{bmatrix} \quad C_{12} = \begin{bmatrix} 4 & 6 \\ 7 & 9 \end{bmatrix} \quad C_{13} = \begin{bmatrix} 4 & 5 \\ 7 & 8 \end{bmatrix} $$
\end{example}
\paragraph{Find the Determinant from a Column} We can find the determinant of a matrix by expanding along a column. We can expand along any row or column, but it is easier to expand along a column with zeros.
\paragraph{Using Elementary Row Operations} We can use elementary row operations to simplify the matrix on the row or column we are expanding along (by having zeros).
\begin{theorem}[Determinant w.r.t Elementary Row Operations]
    Consider the following elementary row operations:
    \begin{enumerate}
        \item \textbf{Swapping two rows}: The determinant of the matrix changes sign due to the alternating property.
        \item \textbf{Multiplying a row by a scalar}: The determinant of the matrix is multiplied by the scalar. This is obvious.
        \item \textbf{Adding a multiple of one row to another row}: The determinant of the matrix remains the same. This is due to the multilinearity property.
    \end{enumerate}
\end{theorem}
\begin{theorem}[Determinant of a Transpose]
    Let $A \in  {^n\mathbb{R}^n}$, then:
    \begin{equation}
        \det(A) = \det(A^T)
    \end{equation}
\end{theorem}
\begin{example}[Determinant of a $5\times5$ Matrix]
    Let $A = \begin{bmatrix} 3 & 2 & 1 & 4 & -1 \\ 5 & 4 & 3 & 2 & 1 \\ 2 & 1 & 2 & 3 & 1 \\ -3 & 4 & 1 & 6 & 7 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix}$. We have:
    \begin{align*}
        \intertext{Step 1: Swap row 1 and row 5}
        \det A &= -\begin{vmatrix} 1 & 2 & 3 & 4 & 5 \\ 5 & 4 & 3 & 2 & 1 \\ 2 & 1 & 2 & 3 & 1 \\ -3 & 4 & 1 & 6 & 7 \\ 3 & 2 & 1 & 4 & -1 \end{vmatrix} \\
        \intertext{Step 2: Get row 2 - 5 start with 0}
        \det A &= -\begin{vmatrix} 1 & 2 & 3 & 4 & 5 \\ 0 & -6 &-12 & -18 & -24 \\ 0 & -3 & -4 & -5 & -9 \\ 0 & 10 & 10 & 18 & 22 \\ 0 & -4 & -8 & -8 & -16 \end{vmatrix} \\
        \intertext{Step 3: Get it to a UTM}
        \det A &= -\begin{vmatrix} 1 & 2 & 3 & 4 & 5 \\ 0 & -6 &-12 & -18 & -24 \\ 0 & 0 & 2 & 4 & 3 \\ 0 & 0 & 0 & 4 & 0 \\ 0 & 0 & 0 & 0 & -3 \end{vmatrix} \\
        \intertext{We can see that each row expansion is just a multiple of the minor below it. Thus, the determinant is the product of the diagonal elements.}
        \det A &= -1 \times -6 \times 2 \times 4 \times -3 = -144
    \end{align*}
\end{example}
\begin{definition}[Singular Matrix]
    Let $A \in  {^n\mathbb{R}^n}$, then $A$ is singular if $\det(A) = 0$.
\end{definition}
\begin{definition}[Adjugate Matrix]
    Let $A \in  {^n\mathbb{R}^n}$, then the adjugate of $A$ is denoted as $\text{adj}(A)$, which is defined as:
    \begin{equation}
        \text{adj}(A) = \begin{bmatrix} C_{11} & -C_{12} & C_{13} & \ldots \\ -C_{21} & C_{22} & -C_{23} & \ldots \\ C_{31} & -C_{32} & C_{33} & \ldots \\ \vdots & \vdots & \vdots & \ddots \end{bmatrix}^T
    \end{equation}
\end{definition}
\begin{theorem}[Determinant and Inverse]
    Let $A \in  {^n\mathbb{R}^n}$, then the following properties hold:
    \begin{enumerate}
        \item $A$ is invertible if and only if $\det(A) \neq 0$.
        \item $A^{-1} = \frac{1}{\det(A)}\text{adj}(A)$.
    \end{enumerate}
\end{theorem}
\subsection{Determinants and Inverses of Elementary Matrices}
\begin{definition}[Elementary Matrices]
    Let $E \in  {^n\mathbb{R}^n}$, then $E$ is an elementary matrix if $E$ is obtained by performing a single elementary row operation on $I$.
\end{definition}
\begin{theorem}[Determinants and Inverses of Elementary Matrices]
    Let $E \in  {^n\mathbb{R}^n}$ be an elementary matrix, then the following properties hold:
    \begin{enumerate}
        \item $\det(E) = 1$.
        \item $E^{-1} = E^T$.
    \end{enumerate}
\end{theorem}
\begin{example}[Determinants and Inverses of Elementary Matrices]
    Let $E_1 = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$, then:
    \begin{align*}
        \det(E_1) &= 1 \times 2 = 2 \\
        E_1^{-1} &= E_1^T = \begin{bmatrix} 1 & 0 \\ 0 & \frac{1}{2} \end{bmatrix}
    \end{align*}
\end{example}
\begin{theorem}[Determinants and Inverses of Product of Matrices]
    Let $A, B \in  {^n\mathbb{R}^n}$, then the following properties hold:
    \begin{enumerate}
        \item $\det(AB) = \det(A)\det(B)$.
        \item $(AB)^{-1} = B^{-1}A^{-1}$.
    \end{enumerate}
\end{theorem}
\begin{theorem}[Derminants and Inverse w.r.t. RREF]
    Let $A \in  {^n\mathbb{R}^n}$, then the following properties hold:
    \begin{enumerate}
        \item $\det(A) = \det(\text{RREF}(A))$.
        \item $A^{-1} = \text{RREF}(A)^{-1}$.
    \end{enumerate}
    However:
    \begin{enumerate}
        \item $\det(\text{RREF}(A)) = \det(A)$.
        \item $\text{RREF}(A)^{-1} \neq A^{-1}$.
    \end{enumerate} 
    
\end{theorem}
\begin{theorem}[The theorem of Determinants and Inverse]
    Let $A \in  {^n\mathbb{R}^n}$, then the following properties hold:
    \begin{enumerate}
        \item $A$ is invertible.
        \item $\det(A) \neq 0$.
        \item $\text{RREF}(A) = I$.
        \item $A$ is a product of elementary matrices.
        \item $\det(A^{-1}) = \frac{1}{\det(A)}$.
        \item $\text{rank}(A) = n$.
    \end{enumerate}
\end{theorem}
\section{Eigen and Diagonalization}
\subsection{Eigenvalues and Eigenvectors}
\begin{definition}[Eigenvalues and Eigenvectors]
    Let $A \in {^n\mathbb{R}^n}$, then $\lambda$ is an eigenvalue of $A$ if there exists a \textbf{non-zero} vector $\textbf{v}$ such that:
    \begin{equation}
        A\textbf{v} = \lambda \textbf{v}
    \end{equation}
    The vector $\textbf{v}$ is called an eigenvector of $A$.
\end{definition}
\begin{theorem}[Properties of Eigenvalues and Eigenvectors]
    Let $A \in {^n\mathbb{R}^n}$, then the following properties hold:
    \begin{enumerate}
        \item The eigenvalues of $A$ are the roots of the characteristic equation:
        \begin{equation}
            \det(A - \lambda I) = 0
        \end{equation}
        \item The eigenvectors of $A$ are the solutions to the system of equations:
        \begin{equation}
            (A - \lambda I)\textbf{v} = \textbf{0}
        \end{equation}
    \end{enumerate}
\end{theorem}
\paragraph{Note} If $A\textbf{v} = \textbf{0}$, then $\textbf{v}$ is an eigenvector of $A$ with eigenvalue $\lambda = 0$.
\begin{example}[Eigenvalues and Eigenvectors]
    Let $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, then the characteristic equation is:
    \begin{align*}
        \det(A - \lambda I) &= \begin{vmatrix} 1-\lambda & 2 \\ 2 & 1-\lambda \end{vmatrix} \\
        &= (1-\lambda)(1-\lambda) - 4 \\
        &= \lambda^2 - 2\lambda - 3 = 0
    \end{align*}
    The roots of the characteristic equation are $\lambda = 3, -1$. The eigenvectors are:
    \begin{align*}
        \text{For } \lambda = 3: \begin{bmatrix} -2 & 2 \\ 2 & -2 \end{bmatrix}\textbf{v} &= \textbf{0} \implies \textbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} \\
        \text{For } \lambda = -1: \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix}\textbf{v} &= \textbf{0} \implies \textbf{v} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}
    \end{align*}
\end{example}
\begin{example}
    Let $A \in {^n\mathbb{R}^n}$ and has eigen value 2 with multiplicity 2, and eigen value 3 with multiplicity 1. Let $x, y, z$ be eigenvectors of $A$ corresponding to the eigenvalues 2, 2, 3 respectively. Then:
    \begin{enumerate}
        \item \textbf{2x is an eigenvector of A}: Since $A(2x) = 2Ax = 2(2x)$, then $2x$ is an eigenvector of $A$ with eigenvalue 2.
        \item \textbf{x + y is an eigenvector of A}: Since $A(x + y) = Ax + Ay = 2x + 2y = 2(x + y)$, then $x + y$ is an eigenvector of $A$ with eigenvalue 2.
        \item \textbf{x + z is an not an eigenvector of A}: Since $A(x + z) = Ax + Az = 2x + 3z \neq \lambda(x + z)$, then $x + z$ is not an eigenvector of $A$.
    \end{enumerate}
\end{example}
\paragraph{0 can be an Eigenvalue} If there exist $\textbf{x} \neq 0$ such that $A\textbf{x} = 0$, then 0 is an eigenvalue of $A$. This require the nullity of $A$ to be greater than 0.
\subsection{Eigenspace and Multiplicity}
\begin{definition}[Eigenspace]
    Let $A \in {^n\mathbb{R}^n}$, then the eigenspace of $A$ corresponding to the eigenvalue $\lambda$ is denoted as $E_\lambda$, which is defined as:
    \begin{equation}
        E_\lambda = \{ \textbf{v} \in \mathbb{R}^n \mid A\textbf{v} = \lambda \textbf{v} \}
    \end{equation}
    , which is the union of all eigenvectors of $A$ corresponding to the eigenvalue $\lambda$ and ${\textbf{0}}$.
\end{definition}
\begin{theorem}[Eigenspace is a Subspace]
    Let $A \in {^n\mathbb{R}^n}$, then the eigenspace of $A$ corresponding to the eigenvalue $\lambda$ is a subspace of $\mathbb{R}^n$ with dimension equal to the multiplicity of $\lambda$ ($\ge 1$).
\end{theorem}
\begin{proof}
    This is obvious as eigenspace is a span of eigenvectors, and the span of eigenvectors is a subspace. To proof it, we use the subspace test:
    \begin{enumerate}
        \item \textbf{Non-emptiness}: Since $\textbf{0}$ satisfies $A\textbf{0} = \lambda \textbf{0}$, then $\textbf{0} \in E_\lambda$.
        \item \textbf{Closure under addition}: Let $\textbf{v}_1, \textbf{v}_2 \in E_\lambda$, then $A(\textbf{v}_1 + \textbf{v}_2) = A\textbf{v}_1 + A\textbf{v}_2 = \lambda \textbf{v}_1 + \lambda \textbf{v}_2 = \lambda(\textbf{v}_1 + \textbf{v}_2)$, then $\textbf{v}_1 + \textbf{v}_2 \in E_\lambda$.
        \item \textbf{Closure under scalar multiplication}: Let $\textbf{v} \in E_\lambda$, then $A(c\textbf{v}) = cA\textbf{v} = c\lambda \textbf{v} = \lambda(c\textbf{v})$, then $c\textbf{v} \in E_\lambda$.
    \end{enumerate}
\end{proof}
\begin{theorem}[Intersection of Eigenspaces]
    Let $A \in {^n\mathbb{R}^n}$, then the intersection of eigenspaces of $A$ corresponding to distinct eigenvalues is $\{\textbf{0}\}$, that is, for eigenvalues $\lambda_1, \lambda_2$, if $\lambda_1 \neq \lambda_2$, then $E_{\lambda_1} \cap E_{\lambda_2} = \{\textbf{0}\}$.
\end{theorem}
\begin{theorem}[Theorem of Eigenvalues]
    The following statements are equivalent:
    \begin{enumerate}
        \item $\lambda$ is an eigenvalue of $A$.
        \item $Ax = \lambda x$ for some non-zero vector $x$.
        \item $\det(A - \lambda I) = 0$.
        \item $A - \lambda I$ is singular.
        \item $A - \lambda I$ has a non-trivial null space.
    \end{enumerate}
    
\end{theorem}
\begin{definition}[Algebraic Multiplicity (AM)]
    Let $A \in {^n\mathbb{R}^n}$, then the algebraic multiplicity of an eigenvalue $\lambda$ is the number of times $\lambda$ appears as a root of the characteristic equation.
\end{definition}
\begin{definition}[Geometric Multiplicity (GM)]
    Let $A \in {^n\mathbb{R}^n}$, then the geometric multiplicity of an eigenvalue $\lambda$ is the dimension of the eigenspace of $\lambda$.
\end{definition}
\begin{theorem}[Multiplicity of Eigenvalues]
    Let $A \in {^n\mathbb{R}^n}$, then the algebraic multiplicity of an eigenvalue is greater than or equal to the geometric multiplicity of the eigenvalue, that is:
    \begin{equation}
        \text{AM}(\lambda) \ge \text{GM}(\lambda) \ge 1
    \end{equation}
\end{theorem}
\begin{theorem}[Product of Eigenvalues]
    Let $A \in \mathbb{R}^{n \times n}$, then the product of the eigenvalues of $A$ is equal to the determinant of $A$:
    \begin{equation}
        \prod_{i=1}^{n} \lambda_i = \det(A)
    \end{equation}
\end{theorem}
\begin{theorem}
    Given $A\in {^n\mathbb{R}^n}$, we can find $P {^n\mathbb{R}^n}$ and $\Lambda \in {^n\mathbb{R}^n}$ where $\Lambda$ is a diagonal matrix so that $A = P\Lambda P^{-1}$ if and only if $A$ has $n$ linearly independent eigenvectors.
\end{theorem}
\begin{theorem}
    $A \in {^n\mathbb{R}^n}$ is diagonalizable if and only if there is a basis for the eigenvectors of $A$ in $\mathbb{R}^n$.
\end{theorem}

\begin{theorem}
    If $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ on the diagonal, then $A$ is similar to $\Lambda$.
\end{theorem}
\begin{lemma}[Linear Independence of Eigenvectors]
    Let $A \in {^n\mathbb{R}^n}$ and $v_1, v_2, \dots v_j$ be eigenvector assocated with distinct eigenvalues, then they are linearly independent.
\end{lemma}
\begin{theorem}
    Let $A \in {^n\mathbb{R}^n}$ and $A$ has $n$ distict eigenvalues, then $A$ is diagonalizable.
\end{theorem}
\begin{theorem}[AM, GM, and Diagonalization]
    Let $A \in {^n\mathbb{R}^n}$, then the following statements are equivalent:
    \begin{enumerate}
        \item $A$ is diagonalizable.
        \item The algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity.
        \item The number of linearly independent eigenvectors of $A$ is equal to the dimension of the eigenspace of $A$.
    \end{enumerate}
\end{theorem}
\paragraph{Similar Matrices} Two matrices $A$ and $B$ are similar if there exists an invertible matrix $P$ such that $A = PBP^{-1}$. This means that $A$ and $B$ have the same eigenvalues and the same characteristic polynomial.
\begin{theorem}[Similar Matrices]
    Let $A, B \in {^n\mathbb{R}^n}$, then the following statements are equivalent:
    \begin{enumerate}
        \item $A$ and $B$ are similar.
        \item $A$ and $B$ have the same eigenvalues.
        \item $A$ and $B$ have the same characteristic polynomial.
    \end{enumerate}
\end{theorem}
\begin{theorem}[Similarity of Charistic Polynomial]
    Let $A, B \in {^n\mathbb{R}^n}$ be similar matrices, then:
    \begin{enumerate}
        \item $\lambda I - A$ and $\lambda I - B$ are similar matrices.
        \item $\det(\lambda I - A) = \det(\lambda I - B)$ (by product of determinants).
        \item $\dim \null (\lambda I - A) = \dim \null (\lambda I - B)$ (by rank-nullity), but not necessarily the same null space. 
    \end{enumerate}
    , thus the characteristic polynomial of $A$ and $B$ are the same and they have the same eigenvalues, which also means that they have the same AM.
\end{theorem}
\begin{theorem}
    If $A$ is a diagonalizable matrix, then $A$ is has equal AM and GM.
\end{theorem}
\begin{proof}
    $A$ is similar to the diagonal matrix $D$, so $\dim null D = \dim null A$. Thus, the AM and GM of $A$ are equal since AM and GM are equal for diagonal matrices.
\end{proof}
\begin{theorem}[Distinct Eigenvalues]
    Let $A \in {^n\mathbb{R}^n}$, then the following statements are equivalent:
    \begin{enumerate}
        \item $A$ has $n$ distinct eigenvalues.
        \item $A$ is diagonalizable.
        \item The algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity.
        \item The number of linearly independent eigenvectors of $A$ is equal to the dimension of the eigenspace of $A$.
    \end{enumerate}
\end{theorem}
\section{Differential Equations}
\begin{definition}
    A system of the form:
    \begin{align*}
        x_1^\prime &= a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n \\
        x_2^\prime &= a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n \\
        \vdots \\
        x_n^\prime &= a_{n1}x_1 + a_{n2}x_2 + \ldots + a_{nn}x_n
    \end{align*}
    , wherer $a_{ij} \in \mathbb{R}$ is a $n \times n$ system of homogeneous linear differential equations with constant coefficients. For $A = \{a_{ij}\}$, we can write the system as:
    \begin{equation}
       X^\prime = AX
    \end{equation}
\end{definition}
\begin{example}[Two Wafers in Breeze]
    Let $\alpha = ha/mc$, $\beta = ka/mcd$, $T = [\hat{T}_1, \hat{T}_2]^T$, wher $h, a, m, c, k, d$ are constants. The system of equations is:
    \begin{align*}
        \frac{d\hat{T}_1}{dt} &= -(\alpha+\beta)\hat{T}_1 + \beta\hat{T}_2 \\
        \frac{d\hat{T}_2}{dt} &= \beta\hat{T}_1 - (\alpha+\beta)\hat{T}_2
    \end{align*}
    Then the coefficients matrix is:
    $$
        A = \begin{bmatrix}
            -(\alpha+\beta) & \beta \\
            \beta & -(\alpha+\beta)
        \end{bmatrix} = \begin{bmatrix} 
            -(ha/mc + ka/mcd) & ka/mcd \\
            ka/mcd & -(ha/mc + ka/mcd)
        \end{bmatrix}
    $$
\end{example}
\begin{example}
    Let:
    $$
    A = \begin{bmatrix}
        17 & -30 \\
        10 & -18 
    \end{bmatrix}
    $$
    , then particular solutions are
    \begin{align*}
        y_1 &= \begin{bmatrix}
            2 \\ 1
        \end{bmatrix}e^{2 t} \\
        y_2 &= \begin{bmatrix}
            3 \\ 2
        \end{bmatrix}e^{3 t}
    \end{align*}
    Our initial conditions for two solutions are:
    \begin{align*}
        y_1(0) &= \begin{bmatrix}
            2 \\ 1
        \end{bmatrix} \\
        y_2(0) &= \begin{bmatrix}
            3 \\ 2
        \end{bmatrix}
    \end{align*}
    , which are the eigenvectors of $A$ corresponding to the eigenvalues $2$ and $3$ respectively. Thus, the intial solution of the system:
    \begin{align*}
        y(0) &= c_1\begin{bmatrix}
            2 \\ 1
        \end{bmatrix} + c_2\begin{bmatrix}
            3 \\ 2
        \end{bmatrix} 
    \end{align*}
    is the span of the eigenvectors of $A$ corresponding to the eigenvalues $2$ and $3$ respectively.
\end{example}
\begin{theorem}
    If $A$ is diagonalizable, then the system of DEs.
    \begin{equation}
        X^\prime = AX
    \end{equation}
    has a basis of solution and we can solve any IVP w.r.t. the system of DEs.
\end{theorem}
\subs
\end{document}